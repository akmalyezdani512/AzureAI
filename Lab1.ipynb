{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Azure AI Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provision an Azure AI Services resource\n",
    "\n",
    "Azure AI Services are cloud-based services that encapsulate artificial intelligence capabilities you can incorporate into your applications. You can provision individual Azure AI services resources for specific APIs (for example, **Language** or **Vision**), or you can provision a single **Azure AI Services** resource that provides access to multiple Azure AI services APIs through a single endpoint and key. In this case, you'll use a single **Azure AI Services** resource.\n",
    "\n",
    "1. Open the Azure portal at `https://portal.azure.com`, and sign in using the Microsoft account associated with your Azure subscription.\n",
    "2. In the top search bar, search for *Azure AI services*, select **Azure AI Services**, and create an Azure AI services multi-service account resource with the following settings:\n",
    "    - **Subscription**: *Your Azure subscription*\n",
    "    - **Resource group**: *Choose or create a resource group (if you are using a restricted subscription, you may not have permission to create a new resource group - use the one provided)*\n",
    "    - **Region**: *Choose any available region*\n",
    "    - **Name**: *Enter a unique name*\n",
    "    - **Pricing tier**: Standard S0\n",
    "3. Select the required checkboxes and create the resource.\n",
    "4. Wait for deployment to complete, and then view the deployment details.\n",
    "5. Go to the resource and view its **Keys and Endpoint** page. This page contains the information that you will need to connect to your resource and use it from applications you develop. Specifically:\n",
    "    - An HTTP *endpoint* to which client applications can send requests.\n",
    "    - Two *keys* that can be used for authentication (client applications can use either key to authenticate).\n",
    "    - The *location* where the resource is hosted. This is required for requests to some (but not all) APIs.\n",
    "\n",
    "## Use a REST Interface\n",
    "\n",
    "The Azure AI services APIs are REST-based, so you can consume them by submitting JSON requests over HTTP. In this example, you'll explore a console application that uses the **Language** REST API to perform language detection; but the basic principle is the same for all of the APIs supported by the Azure AI Services resource.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "COG_SERVICE_ENDPOINT=\"https://xxxxxxx.cognitiveservices.azure.com/\"\n",
    "COG_SERVICE_KEY=\"xxxxxxxxxxxxxx\"\n",
    "region = \"westus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import http.client, base64, json, urllib\n",
    "from urllib import request, parse, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"documents\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"gracias\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = input (\"Enter some text\")\n",
    "jsonBody = {\n",
    "            \"documents\":[\n",
    "                {\"id\": 1,\n",
    "                 \"text\": text}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Let's take a look at the JSON we'll send to the service\n",
    "print(json.dumps(jsonBody, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"documents\":[{\"id\":\"1\",\"detectedLanguage\":{\"name\":\"Spanish\",\"iso6391Name\":\"es\",\"confidenceScore\":1.0},\"warnings\":[]}],\"errors\":[],\"modelVersion\":\"2024-04-01\"}\n"
     ]
    }
   ],
   "source": [
    "# Make an HTTP request to the REST interface\n",
    "uri = COG_SERVICE_ENDPOINT.rstrip('/').replace('https://', '')\n",
    "conn = http.client.HTTPSConnection(uri)\n",
    "\n",
    "# Add the authentication key to the request header\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Ocp-Apim-Subscription-Key': COG_SERVICE_KEY\n",
    "}\n",
    "\n",
    "# Use the Text Analytics language API\n",
    "conn.request(\"POST\", \"/text/analytics/v3.1/languages?\", str(jsonBody).encode('utf-8'), headers)\n",
    "\n",
    "# Send the request\n",
    "response = conn.getresponse()\n",
    "data = response.read().decode(\"UTF-8\")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"documents\": [\n",
      "    {\n",
      "      \"id\": \"1\",\n",
      "      \"detectedLanguage\": {\n",
      "        \"name\": \"Spanish\",\n",
      "        \"iso6391Name\": \"es\",\n",
      "        \"confidenceScore\": 1.0\n",
      "      },\n",
      "      \"warnings\": []\n",
      "    }\n",
      "  ],\n",
      "  \"errors\": [],\n",
      "  \"modelVersion\": \"2024-04-01\"\n",
      "}\n",
      "\n",
      "Language: Spanish\n"
     ]
    }
   ],
   "source": [
    "# If the call was successful, get the response\n",
    "if response.status == 200:\n",
    "\n",
    "    # Display the JSON response in full (just so we can see it)\n",
    "    results = json.loads(data)\n",
    "    print(json.dumps(results, indent=2))\n",
    "\n",
    "    # Extract the detected language name for each document\n",
    "    for document in results[\"documents\"]:\n",
    "        print(\"\\nLanguage:\", document[\"detectedLanguage\"][\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use an SDK\n",
    "\n",
    "You can write code that consumes Azure AI services REST APIs directly, but there are software development kits (SDKs) for many popular programming languages, including Microsoft C#, Python, and Node.js. Using an SDK can greatly simplify development of applications that consume Azure AI services.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anshu pandey\\appdata\\roaming\\python\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-ai-textanalytics==5.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client using endpoint and key\n",
    "credential = AzureKeyCredential(COG_SERVICE_KEY)\n",
    "client = TextAnalyticsClient(endpoint=COG_SERVICE_ENDPOINT, credential=credential)\n",
    "\n",
    "# Call the service to get the detected language\n",
    "detectedLanguage = client.detect_language(documents = [text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish\n"
     ]
    }
   ],
   "source": [
    "print(detectedLanguage.primary_language.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import namespaces\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client using endpoint and key\n",
    "credential = AzureKeyCredential(COG_SERVICE_KEY)\n",
    "cog_client = TextAnalyticsClient(endpoint=COG_SERVICE_ENDPOINT, credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apa Kabar!\n"
     ]
    }
   ],
   "source": [
    "text = \"Apa Kabar!\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Language: Indonesian\n"
     ]
    }
   ],
   "source": [
    "# Get language\n",
    "detectedLanguage = cog_client.detect_language(documents=[text])[0]\n",
    "print('\\nLanguage: {}'.format(detectedLanguage.primary_language.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate sentiment\n",
    "\n",
    "Sentiment analysis is a commonly used technique to classify text as positive or negative (or possible neutral or mixed). It's commonly used to analyze social media posts, product reviews, and other items where the sentiment of the text may provide useful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was amazing and I loved it. Charles did a fantastic job here.\n"
     ]
    }
   ],
   "source": [
    "text = \"The movie was amazing and I loved it. Charles did a fantastic job here.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# Get sentiment\n",
    "sentimentAnalysis = cog_client.analyze_sentiment(documents=[text])[0]\n",
    "print(\"\\nSentiment: {}\".format(sentimentAnalysis.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0.99, 'neutral': 0.01, 'negative': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(sentimentAnalysis.confidence_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify key phrases\n",
    "It can be useful to identify key phrases in a body of text to help determine the main topics that it discusses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In recent years, the field of artificial intelligence (AI) has seen significant advancements, particularly in the development \n",
      "of machine learning algorithms and natural language processing (NLP) techniques. \n",
      "Companies like Google, IBM, and Microsoft have been at the forefront of these innovations, investing heavily in research and development.\n",
      "\n",
      "One of the most notable breakthroughs is the creation of deep learning models, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). \n",
      "These models have enabled substantial improvements in image and speech recognition tasks, leading to applications in various industries, including healthcare, finance, and autonomous driving.\n",
      "\n",
      "In healthcare, AI-powered diagnostic tools are revolutionizing the way diseases are detected and treated. For example, AI algorithms can analyze medical images to \n",
      "identify early signs of cancer, improving patient outcomes through early intervention. In finance, machine learning models are used for fraud detection and to provide personalized financial advice to customers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\" In recent years, the field of artificial intelligence (AI) has seen significant advancements, particularly in the development \n",
    "of machine learning algorithms and natural language processing (NLP) techniques. \n",
    "Companies like Google, IBM, and Microsoft have been at the forefront of these innovations, investing heavily in research and development.\n",
    "\n",
    "One of the most notable breakthroughs is the creation of deep learning models, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). \n",
    "These models have enabled substantial improvements in image and speech recognition tasks, leading to applications in various industries, including healthcare, finance, and autonomous driving.\n",
    "\n",
    "In healthcare, AI-powered diagnostic tools are revolutionizing the way diseases are detected and treated. For example, AI algorithms can analyze medical images to \n",
    "identify early signs of cancer, improving patient outcomes through early intervention. In finance, machine learning models are used for fraud detection and to provide personalized financial advice to customers.\n",
    "\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key Phrases:\n",
      "\tnatural language processing\n",
      "\tconvolutional neural networks\n",
      "\trecurrent neural networks\n",
      "\tspeech recognition tasks\n",
      "\tAI-powered diagnostic tools\n",
      "\tpersonalized financial advice\n",
      "\tmachine learning algorithms\n",
      "\tdeep learning models\n",
      "\tmachine learning models\n",
      "\trecent years\n",
      "\tartificial intelligence\n",
      "\tsignificant advancements\n",
      "\tNLP) techniques\n",
      "\tnotable breakthroughs\n",
      "\tsubstantial improvements\n",
      "\tvarious industries\n",
      "\tautonomous driving\n",
      "\tAI algorithms\n",
      "\tmedical images\n",
      "\tearly signs\n",
      "\tpatient outcomes\n",
      "\tearly intervention\n",
      "\tfraud detection\n",
      "\tfield\n",
      "\tdevelopment\n",
      "\tCompanies\n",
      "\tGoogle\n",
      "\tIBM\n",
      "\tMicrosoft\n",
      "\tforefront\n",
      "\tinnovations\n",
      "\tresearch\n",
      "\tcreation\n",
      "\tCNNs\n",
      "\tRNNs\n",
      "\tapplications\n",
      "\thealthcare\n",
      "\tfinance\n",
      "\tway\n",
      "\tdiseases\n",
      "\texample\n",
      "\tcancer\n",
      "\tcustomers\n"
     ]
    }
   ],
   "source": [
    "# Get key phrases\n",
    "phrases = cog_client.extract_key_phrases(documents=[text])[0].key_phrases\n",
    "if len(phrases) > 0:\n",
    "    print(\"\\nKey Phrases:\")\n",
    "    for phrase in phrases:\n",
    "        print('\\t{}'.format(phrase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract entities\n",
    "\n",
    "Often, documents or other bodies of text mention people, places, time periods, or other entities. The text Analytics API can detect multiple categories (and subcategories) of entity in your text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " On June 15, 2023, John Smith, a software engineer at Google, Inc., traveled from San Francisco to New York City for a conference on Artificial Intelligence. \n",
      "The event was held at the Javits Center and sponsored by IBM and Microsoft. \n",
      "During his stay, John stayed at the Hilton Hotel and dined at the renowned restaurant Eleven Madison Park. \n",
      "He met with Dr. Jane Doe, a leading researcher from Stanford University, to discuss advancements in natural language processing.\n",
      "\n",
      "In his free time, John visited the Metropolitan Museum of Art and took a ferry to see the Statue of Liberty. \n",
      "He also attended a Broadway show, \"Hamilton,\" and purchased a souvenir from a local shop. \n",
      "On the flight back, he read the latest issue of The New York Times and finished a book titled \"The Innovators\" by Walter Isaacson.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\" On June 15, 2023, John Smith, a software engineer at Google, Inc., traveled from San Francisco to New York City for a conference on Artificial Intelligence. \n",
    "The event was held at the Javits Center and sponsored by IBM and Microsoft. \n",
    "During his stay, John stayed at the Hilton Hotel and dined at the renowned restaurant Eleven Madison Park. \n",
    "He met with Dr. Jane Doe, a leading researcher from Stanford University, to discuss advancements in natural language processing.\n",
    "\n",
    "In his free time, John visited the Metropolitan Museum of Art and took a ferry to see the Statue of Liberty. \n",
    "He also attended a Broadway show, \"Hamilton,\" and purchased a souvenir from a local shop. \n",
    "On the flight back, he read the latest issue of The New York Times and finished a book titled \"The Innovators\" by Walter Isaacson.\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entities\n",
      "\tartificial intelligence (Skill)\n",
      "\tAI (Skill)\n",
      "\tdevelopment (Skill)\n",
      "\tmachine learning algorithms (Skill)\n",
      "\tnatural language processing (Skill)\n",
      "\tNLP (Skill)\n",
      "\tGoogle (Organization)\n",
      "\tIBM (Organization)\n",
      "\tMicrosoft (Organization)\n",
      "\tinnovations (Skill)\n",
      "\tresearch (Skill)\n",
      "\tdevelopment (Skill)\n",
      "\tOne (Quantity)\n",
      "\tdeep learning (Skill)\n",
      "\timage (Skill)\n",
      "\tspeech recognition (Skill)\n",
      "\thealthcare (Skill)\n",
      "\tfinance (Skill)\n",
      "\tAI (Skill)\n",
      "\tdiagnostic tools (Product)\n",
      "\tAI (Skill)\n",
      "\talgorithms (Product)\n",
      "\timages (Product)\n",
      "\tpatient (PersonType)\n",
      "\tintervention (Skill)\n",
      "\tfinance (Skill)\n",
      "\tmachine learning (Skill)\n",
      "\tfinancial (Skill)\n",
      "\tcustomers (PersonType)\n"
     ]
    }
   ],
   "source": [
    "# Get entities\n",
    "entities = cog_client.recognize_entities(documents=[text])[0].entities\n",
    "if len(entities) > 0:\n",
    "    print(\"\\nEntities\")\n",
    "    for entity in entities:\n",
    "        print('\\t{} ({})'.format(entity.text, entity.category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract linked entities\n",
    "In addition to categorized entities, the Text Analytics API can detect entities for which there are known links to data sources, such as Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Links\n",
      "\tJune 15 (https://en.wikipedia.org/wiki/June_15)\n",
      "\tJohn Doe (https://en.wikipedia.org/wiki/John_Doe)\n",
      "\tGoogle (https://en.wikipedia.org/wiki/Google)\n",
      "\tSan Francisco (https://en.wikipedia.org/wiki/San_Francisco)\n",
      "\tNew York City (https://en.wikipedia.org/wiki/New_York_City)\n",
      "\tArtificial intelligence (https://en.wikipedia.org/wiki/Artificial_intelligence)\n",
      "\tJavits Center (https://en.wikipedia.org/wiki/Javits_Center)\n",
      "\tIBM (https://en.wikipedia.org/wiki/IBM)\n",
      "\tMicrosoft (https://en.wikipedia.org/wiki/Microsoft)\n",
      "\tNew York Hilton Midtown (https://en.wikipedia.org/wiki/New_York_Hilton_Midtown)\n",
      "\tEleven Madison Park (https://en.wikipedia.org/wiki/Eleven_Madison_Park)\n",
      "\tStanford University (https://en.wikipedia.org/wiki/Stanford_University)\n",
      "\tMetropolitan Museum of Art (https://en.wikipedia.org/wiki/Metropolitan_Museum_of_Art)\n",
      "\tStatue of Liberty (https://en.wikipedia.org/wiki/Statue_of_Liberty)\n",
      "\tBroadway theatre (https://en.wikipedia.org/wiki/Broadway_theatre)\n",
      "\tHamilton (musical) (https://en.wikipedia.org/wiki/Hamilton_(musical))\n",
      "\tThe New York Times (https://en.wikipedia.org/wiki/The_New_York_Times)\n",
      "\tThe Innovators (book) (https://en.wikipedia.org/wiki/The_Innovators_(book))\n",
      "\tWalter Isaacson (https://en.wikipedia.org/wiki/Walter_Isaacson)\n"
     ]
    }
   ],
   "source": [
    "# Get linked entities\n",
    "entities = cog_client.recognize_linked_entities(documents=[text])[0].entities\n",
    "if len(entities) > 0:\n",
    "    print(\"\\nLinks\")\n",
    "    for linked_entity in entities:\n",
    "        print('\\t{} ({})'.format(linked_entity.name, linked_entity.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate Text\n",
    "Azure AI Translator is a service that enables you to translate text between languages.\n",
    "\n",
    "For example, suppose a travel agency wants to examine hotel reviews that have been submitted to the company's web site, standardizing on English as the language that is used for analysis. By using Azure AI Translator, they can determine the language each review is written in, and if it is not already English, translate it from whatever source language it was written in into English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect language\n",
    "Azure AI Translator can automatically detect the source language of text to be translated, but it also enables you to explicitly detect the language in which text is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "שלום לכולם,\n",
    "מה שלומכם?\n",
    "תודה\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'isTranslationSupported': True, 'isTransliterationSupported': True, 'language': 'he', 'score': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# Use the Azure AI Translator detect function\n",
    "\n",
    "translator_endpoint = 'https://api.cognitive.microsofttranslator.com'\n",
    "\n",
    "path = '/detect'\n",
    "url = translator_endpoint + path\n",
    "\n",
    "# Build the request\n",
    "params = {\n",
    "    'api-version': '3.0'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "'Ocp-Apim-Subscription-Key': COG_SERVICE_KEY,\n",
    "'Ocp-Apim-Subscription-Region': region,\n",
    "'Content-type': 'application/json'\n",
    "}\n",
    "\n",
    "body = [{\n",
    "    'text': text\n",
    "}]\n",
    "\n",
    "# Send the request and get response\n",
    "request = requests.post(url, params=params, headers=headers, json=body)\n",
    "response = request.json()\n",
    "\n",
    "# Parse JSON array and get language\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate text\n",
    "Now that your application can determine the language in which reviews are written, you can use Azure AI Translator to translate any non-English reviews into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure AI Translator translate function\n",
    "path = '/translate'\n",
    "url = translator_endpoint + path\n",
    "\n",
    "# Build the request\n",
    "params = {\n",
    "    'api-version': '3.0',\n",
    "    'from': 'he',\n",
    "    'to': ['en','id','fr','es']\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': COG_SERVICE_KEY,\n",
    "    'Ocp-Apim-Subscription-Region': region,\n",
    "    'Content-type': 'application/json'\n",
    "}\n",
    "\n",
    "body = [{\n",
    "    'text': text\n",
    "}]\n",
    "\n",
    "# Send the request and get response\n",
    "request = requests.post(url, params=params, headers=headers, json=body)\n",
    "response = request.json()\n",
    "\n",
    "# Parse JSON array and get translation\n",
    "translation = response[0][\"translations\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Hello everyone,\n",
      "How are you?\n",
      "Thank you\n"
     ]
    }
   ],
   "source": [
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Hello everyone,\n",
      "How are you?\n",
      "Thank you\n",
      " \n",
      "Halo semuanya,\n",
      "Bagaimana keadaanmu?\n",
      "Terima kasih\n",
      " \n",
      "Bonjour à tous,\n",
      "Comment vas-tu?\n",
      "Merci\n",
      " \n",
      "Hola a todos,\n",
      "¿Cómo estás?\n",
      "Gracias\n"
     ]
    }
   ],
   "source": [
    "for output in response[0]['translations']:\n",
    "    print(output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
